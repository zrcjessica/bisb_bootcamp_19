{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM tutorial\n",
    "\n",
    "Here, we will implement a simple binomial GLM using data for California standardized testing results (STAR testing, for those of you who grew up here) for grades 2-11. This dataset consists of results for 303 unified school districts, and the binary response variable represents the number of 9th graders scoring above the national median for the math section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for tutorial \n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset from the `statsmodels` package\n",
    "\n",
    "More information about `statsmodels` datasets can be found [here](https://www.statsmodels.org/devel/datasets/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.datasets.star98.NOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down:\n",
    "\n",
    "- `NABOVE` and `NBELOW` represent the binary response variable\n",
    "- `LOWINC` through `PCTYRRND` represent the **12** independent variables, or regressors. These would be represented by $\\mathbf X_1,...,\\mathbf X_n$ for $n$ regressors.\n",
    "- There are **8** interaction terms, representing non-linear interactions between two or more regressors. When an interaction is present, the effect of a regressor on the response variable depends on the value(s) of the variable(s) which it interacts with. The values of these interaction variables is simply the product of its interacting terms; for example, if variables $A$ and $B$ are interacting, the value of its interaction term, $\\mathbf X_{AB} = \\mathbf X_A \\circ \\mathbf X_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's preview the dataset as a `pandas` dataframe (via `statsmodels` functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sm.datasets.star98.load_pandas()\n",
    "data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statsmodels, `endog` refers to the response variable(s). Here, it will return `NABOVE` and `NBELOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.endog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`exog` refers to all of the other independent variables/regressors/interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.exog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Let's visualize the data and get a high-level idea of what's going on. To start, let's use seaborn to visualize the relationship between each of the independent variables, excluding the interaction terms, with the percentage of 9th grade students in each district scoring above the national median on the math section of the STAR test (100*`NABOVE`/(`NABOVE`+`NBELOW`)). \n",
    "\n",
    "It will be convenient to first reformat the data table into a form that is more friendly for exploratory data visualization. If you recall, each column represents a variable. We will reshape the table such that for each of the response variables (`id_vars`), there is one entry for each of the regressors (`value_vars`). By default, any columns not set as `id_vars` will be interpreted as `value_vars`. Each sample will have a record of the variable name and value. This process is called [**melting**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html) the dataframe.\n",
    "\n",
    "Here, we will subset the data table to the columns containing the binary response variables (`NABOVE` and `NBELOW`) and the first 12 variables (excluding the interaction terms). This corresponds to the first 14 columns of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = data.data.iloc[:,:14].melt(id_vars = ['NABOVE','NBELOW'])\n",
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add columns corresponding to the percentage of students above the national median (`PCTABOVE`) and the percentage of students below the national median (`PCTBELOW`).\n",
    "\n",
    "Then we will use `seaborn` to generate [line plots](https://seaborn.pydata.org/generated/seaborn.regplot.html) relating `PCTABOVE` to each of the 12 independent variables selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['PCTABOVE'] = 100*plot_df['NABOVE']/(plot_df['NABOVE']+plot_df['NBELOW'])\n",
    "plot_df['PCTBELOW'] = 100*plot_df['NBELOW']/(plot_df['NABOVE']+plot_df['NBELOW'])\n",
    "\n",
    "sns.relplot(x = 'value', y = 'PCTABOVE', hue = 'variable', kind='line',\n",
    "             col = 'variable', col_wrap=3, aspect=1, data = plot_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, we can do the same thing, but with `PCTBELOW` on the y-axis this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x = 'value', y = 'PCTBELOW', hue = 'variable', kind='line',\n",
    "             col = 'variable', col_wrap=3, aspect=1, data = plot_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the distributions of the values of the independent variables (excluding interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.boxplot(x='variable',y='value',data = data.exog.iloc[:,:12].melt())\n",
    "plt.title('Distribution of explanatory variables (without interactions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "The response variable is binomially distributed - this means you have a discrete, non-negative number of \"successes\" and a discrete, non-negative number of \"failures\". Here, this corresponds to `NABOVE` and `NBELOW`.\n",
    "\n",
    "##### Parameterization\n",
    "The binomial distribution is parameterized by $n\\in \\mathbb N$ and $p\\in [0,1]$, and is expressed as $X\\sim\\text{Binomial}(n,p)$, where $X$ is a binomial random variable.\n",
    "\n",
    "##### PMF\n",
    "The probability mass function expresses the probability of getting $k$ successes in $n$ trials:\n",
    "$$p(k,n,p)=\\Pr(k;n,p)=\\Pr(X=k)={n\\choose k}p^k(1-p)^{n-k}$$\n",
    "for $k=0,1,2,...,n,$ where ${n\\choose k}$ is the binomial coefficient:\n",
    "$${n\\choose k}=\\frac{n!}{k!(n-k)!}.$$\n",
    "\n",
    "Essentially:\n",
    "- $k$ successes occur with probability $p^k$, and\n",
    "- $n-k$ failures occur with probability $(1-p)^{n-k}$\n",
    "\n",
    "Becasue the $k$ successes can occur at any point within the $n$ trials, there are ${n\\choose k}$ different ways of observing $k$ successes in $n$ trials.\n",
    "\n",
    "### Link function for the Binomial distribution - [Binary Logistic Regression](https://newonlinecourses.science.psu.edu/stat504/node/216/)\n",
    "\n",
    "The Binomial GLM can be expressed as:\n",
    "$$y_i \\sim \\text{Binomial}(n_i,p_i)$$\n",
    "\n",
    "with the logit link function:\n",
    "$$p_i = \\text{logit} ^{-1}(\\mathbf X \\beta) = \\frac{\\exp(\\mathbf X \\beta)}{1+\\exp(\\mathbf X \\beta)} = \\frac{1}{1+\\exp(-\\mathbf X \\beta)}$$\n",
    "\n",
    "### Implement the GLM\n",
    "We need to estimate values of $\\beta$ corresponding to each of the variables in $\\mathbf X$, to give an estimate of the effect size of each predictor - that is, a measure of the effect that they have on the binomial response variable, which in this case is `NABOVE`.\n",
    "\n",
    "For each sample (district) in the dataset, we have $k_i$ (`NABOVE`) and $n_i$ (`NABOVE`+`NBELOW`). We can estimate $p_i$ using the link function above, as we have the values of $\\mathbf X$ from the dataset in `data.exog`. The values of $\\beta$ giving the value of $p_i$ that defines a distribution shape that best fits to the data - comprised of $\\mathbf X$ and $k_i =$`NABOVE` - are to be estimated by minimizing the log-likelihood function.\n",
    "\n",
    "##### [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)\n",
    "We will minimize the negative log-likelihood. Here, we will use the scipy implementation of [`logpmf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html) and the parameters $n,k,p$ to obtain the log-likelihood. The log-likelihood function can be defined as:\n",
    "\n",
    "$$\\log (\\mathcal{L}(n,k,p\\mid \\mathbf{x}))=\\sum_{i=1}^{N} \\log (p(x_i\\mid n,k,p)) = \\sum_{i=1}^{N} \\log (P(X=x_i\\mid n,k,p)),$$\n",
    "\n",
    "where,\n",
    "- $n,k,p$ are the parameters defining the shape of the binomial distribution,\n",
    "- $N$ represents the total number of school districts (303),\n",
    "- $x_i$ represents `NABOVE` for district $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the negative log-likelihood function\n",
    "def loglik(betas, x, y):\n",
    "    '''betas = effect sizes of variables (parameters to estimate)\n",
    "    x = values of variables\n",
    "    y = NABOVE and NBELOW'''\n",
    "    \n",
    "    nTrials = y.NABOVE + y.NBELOW\n",
    "    pSuccess = (1+np.exp(-np.dot(x,betas)))**-1\n",
    "    ll = stats.binom.logpmf(k = y.NABOVE, \n",
    "                            n = nTrials,\n",
    "                            p = pSuccess)\n",
    "    return -ll.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "For this example, we will restrict the number of variables to just the first 5 independent variables. This is because the optimizer can struggle with a large parameter space (a lot of $\\beta$ values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model with first 5 variables interactions\n",
    "res_five = optimize.minimize(fun = loglik,\n",
    "                            x0 = np.zeros(5),\n",
    "                            args = (data.exog.iloc[:,:5], data.endog),\n",
    "                                      method = 'Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the values of those $\\beta$ estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = res_five.x, y = list(data.exog)[:5])\n",
    "plt.xlabel(r'$\\beta_i$', fontsize=16)\n",
    "plt.title('parameter estimates for 5 variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we fit the model with an additional variable - this time, we'll use the first 6 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with first 6 variables interactions\n",
    "res_six = optimize.minimize(fun = loglik,\n",
    "                            x0 = np.zeros(6),\n",
    "                            args = (data.exog.iloc[:,:6], data.endog),\n",
    "                                      method= 'Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = res_six.x, y = list(data.exog)[:6])\n",
    "plt.xlabel(r'$\\beta_i$', fontsize=16)\n",
    "plt.title('parameter estimates for 6 models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does including another variable improve the model fit? \n",
    "\n",
    "We can use the [**likelihood-ratio test**](https://en.wikipedia.org/wiki/Likelihood-ratio_test) to determine whether the model with six variables fits the data better than the model with five variables. \n",
    "\n",
    "$H_0$: The model with six variables does not fit the data significantly better than the model with five variables.\n",
    "\n",
    "$H_A$: The model with six variables fits the data significantly better than the model with five variables.\n",
    "\n",
    "The likelihood ratio can be computed as:\n",
    "\n",
    "$$ LR = -2\\ln{\\left( \\frac{\\mathcal L(\\theta_0)}{\\mathcal L(\\theta_A)} \\right)} = -2\\left(\\ln{\\left(\\mathcal L(\\theta_0)\\right)}-\\ln{\\left(\\mathcal L(\\theta_A)\\right)}\\right)$$\n",
    "\n",
    "Because $LR$ is [$\\chi^2$-distributed](https://en.wikipedia.org/wiki/Chi-squared_distribution), we can use this property to determine the p-value of the LRT:\n",
    "\n",
    "$$ p = 1-\\text{CDF}_k(LR), $$\n",
    "\n",
    "where $k$ is the degrees of freedom, or the difference in the number of parameters for $H_0$ and $H_A$. In our example, $k = 6-5 = 1$. To compute the p-value, we will use the scipy implementation of the survival function, [`sf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood-ratio test\n",
    "def LRT(loglik_null, loglik_alt, nparams_null, nparams_alt):\n",
    "    df = nparams_alt - nparams_null\n",
    "    lr = -2*(loglik_null - loglik_alt)\n",
    "    p = stats.chi2.sf(lr, df)\n",
    "    return (lr, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've defined the `LRT` function to take the log-likelihoods of $H_0$ and $H_A$, but our minimization functions computed the _negative_ log-likelihoods, we have to make sure to make this correction when passing those values to `LRT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, p = LRT(-res_five.fun, -res_six.fun, 5, 6)\n",
    "print('LR = %.3g, p = %.3g' % (lr, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss what this means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statsmodels GLM implementation\n",
    "\n",
    "To see how we did, let's do exactly what we did above, but with the builtin [statsmodels implementation of GLMs](https://www.statsmodels.org/stable/glm.html). Let's compare the results for fitting 6 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_binom = sm.GLM(data.endog, data.exog.iloc[:,:6], family=sm.families.Binomial())\n",
    "res = glm_binom.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = res.params, y = list(data.exog)[:6])\n",
    "plt.xlabel(r'$\\beta_i$', fontsize=16)\n",
    "plt.title('parameter estimates for 6 variables\\nstatsmodels GLM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = res_six.x, y = res.params)\n",
    "plt.xlabel('our GLM', fontsize = 14)\n",
    "plt.ylabel('statsmodels GLM', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do our estimates compare to these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also fit a GLM to the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\n",
    "res.full = glm_binom.fit()\n",
    "print(res.full.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = res.full.params, y = list(data.exog))\n",
    "plt.xlabel(r'$\\beta_i$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
