{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for normally distributed data\n",
    "\n",
    "Let's walk through an example of using MLE to estimate the parameters for the distribution of some sample data. To start, let's [simulate some data from a normal distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) with known parameters, which we will refer to as $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 4\n",
    "SIGMA = 3\n",
    "N = 1000\n",
    "norm_sample = stats.norm.rvs(loc = MU, scale = SIGMA, size = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Let's take a look at what our sample data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(norm_sample)\n",
    "plt.xlabel('x')\n",
    "plt.title('distribution of Gaussian random sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define likelihood function\n",
    "\n",
    "As you recall, the [likelihood function for a continuous distribution](https://en.wikipedia.org/wiki/Likelihood_function#Continuous_probability_distribution) such as this one:\n",
    "\n",
    "$$\\mathcal{L}(\\theta\\mid \\mathbf{x})=f_{\\theta}(\\mathbf{x})=P_{\\theta}(X=\\mathbf{x})$$\n",
    "or,\n",
    "$$\\mathcal{L}(\\theta\\mid \\mathbf{x})=\\prod_{i=1}^{N} f_{\\theta}(x_i) = \\prod_{i=1}^{N} P_{\\theta}(X=x_i).$$\n",
    "\n",
    "$f_{\\theta}(x_i)=P_{\\theta}(X=x)$ is essentially another way of writing the PDF for the continuous distribution given the parameterization $\\theta$. \n",
    "\n",
    "Here, because we are fitting the data to a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), $\\theta=[\\mu,\\sigma]$. The PDF for the normal distribution is defined as:\n",
    "\n",
    "$$f_{\\mu,\\sigma}(x)=f(x\\mid\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)}.$$\n",
    "\n",
    "In practice, it is more common to optimize over the log-likelihood, because the product of many probability values can lead to floating point errors in computation:\n",
    "\n",
    "$$\\log (\\mathcal{L}(\\theta\\mid \\mathbf{x}))=\\sum_{i=1}^{N} \\log (f_{\\theta}(x_i)) = \\sum_{i=1}^{N} \\log (P_{\\theta})(X=x_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize \n",
    "\n",
    "Many built-in optimizers don't have a maximization function, only a minimization function. Therefore, instead of maximizing the log-likelihood, we will minimize the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define negative log-likelihood function\n",
    "def ll_norm(params, x):\n",
    "    '''params is a list of parameters to estimate: [mu, sigma]\n",
    "    x is list of normally distributed values described by estimated params'''\n",
    "    \n",
    "    mu = params[0]\n",
    "    sigma = params[1]\n",
    "    loglik = np.log((1/((2*math.pi*sigma**2)**0.5))*np.exp(-((x-mu)**2)/(2*sigma**2)))\n",
    "    return -loglik.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick intro to essential arguments for [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html):\n",
    "- `fun`: function to minimize\n",
    "- `x0`: initial guesses for parameters to estimate, in the form of a list, tuple, or numpy array\n",
    "- `args`: any other variables to pass to the function, given in a list, tuple, or numpy array\n",
    "- `bounds`: bounds for parameters to be estimated, given as a list of lists or tuple of tuples, corresponds to params in `x0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize negative log-likelihood\n",
    "norm_res = optimize.minimize(fun = ll_norm,\n",
    "              x0 = [0,1],\n",
    "              args = (norm_sample),\n",
    "            bounds = ((None,None),(1e-6,None)))\n",
    "\n",
    "norm_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpreting the results \n",
    "- `fun`: minimum value of function at estimated parameters\n",
    "- `nfev`: number of function evaluations\n",
    "- `nit`: number of iterations\n",
    "- `success`: bool - did the optimizer run into an issue?\n",
    "- `x`: array of estimated parameters that minimize the function, corresponding to `x0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mu = %d, sigma = %d' % (MU, SIGMA))\n",
    "print('mu_est = %.4f, sigma_est = %.4f' % (norm_res.x[0], norm_res.x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the estimated parameters $\\hat\\mu$ and $\\hat\\sigma$ compare to the actual parameter values used to generate the sample data? How will changing the size of the sample affect the precision of the estimation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for Poisson distributed data\n",
    "\n",
    "Now let's see how we can use MLE to estimate the parameters of the distribution from which a set of [Poisson distributed data](https://en.wikipedia.org/wiki/Poisson_distribution) came from. Poisson-distributed data is discrete and nonnegative. The Poisson distribution takes only one parameter, $\\lambda>0$, which represents the rate at which events occur. \n",
    "\n",
    "It is a discrete probability distribution that expresses the probability of a given number of events, $k$, occurring in a fixed interval of time or space, if the events occur with a known constant rate $\\lambda$ and independently of the time since the last event. \n",
    "\n",
    "Here, we will randomly generate a set of Poisson distributed data for a specified value of $\\lambda$.\n",
    "\n",
    "Note: The [scipy notation for the Poisson distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html) uses $\\mu$ in place of $\\lambda$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 0.5\n",
    "N = 1000\n",
    "poisson_sample = stats.poisson.rvs(mu = LAMBDA, size = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(poisson_sample)\n",
    "plt.xlabel('k')\n",
    "plt.title('distribution of Poisson random sample')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define likelihood function\n",
    "\n",
    "For a [discrete distribution](https://en.wikipedia.org/wiki/Likelihood_function#Discrete_probability_distribution) such as the Poisson, the likelihood function doesn't change much, except it takes the products of the probability mass functions (PMF) rather than the probability distribution function (PDF):\n",
    "\n",
    "$$\\mathcal{L}(\\theta\\mid \\mathbf{x})=p_{\\theta}(\\mathbf{x})=P_{\\theta}(X=\\mathbf{x})$$\n",
    "or,\n",
    "$$\\mathcal{L}(\\theta\\mid \\mathbf{x})=\\prod_{i=1}^{N} p_{\\theta}(x_i) = \\prod_{i=1}^{N} P_{\\theta}(X=x_i).$$\n",
    "\n",
    "Again, we will need to define the [PMF for the Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution#Probability_of_events_for_a_Poisson_distribution) to define our likelihood function. Because the Poisson is parameterized by $\\lambda$, $\\theta=[\\lambda]$ for our likelihood function.\n",
    "$$p_{\\lambda}(k)=p(k \\mid \\lambda)=\\exp^{-\\lambda}\\frac{\\lambda^k}{k!}$$\n",
    "\n",
    "Again, we will minimize over the negative log-likelihood function. The log-likelihood function for the Poisson is expressed as:\n",
    "$$\\log (\\mathcal{L}(\\theta\\mid \\mathbf{k}))=\\sum_{i=1}^{N} \\log (p_{\\theta}(k_i)) = \\sum_{i=1}^{N} \\log (P_{\\theta}(X=k_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_pois(params, k):\n",
    "    '''params is list of parameters to estimate: [lambda]\n",
    "    k is list of Poisson distributed values described by estimated parameter'''\n",
    "    \n",
    "    lmbd = params[0]\n",
    "    loglik = np.log(np.exp(-lmbd)*(lmbd**k)/factorial(k))\n",
    "    return -loglik.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When you want to set bounds but you only have one parameter to estimate, you need to format it as demonstrated below, or you will run into an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_res = optimize.minimize(fun = ll_pois,\n",
    "              x0 = [1e-6],\n",
    "              args = (poisson_sample),\n",
    "            bounds = ((0,None),))\n",
    "\n",
    "pois_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lambda = %.1f' % LAMBDA)\n",
    "print('lambda_est = %.4f' % pois_res.x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy has built-in [pdf](https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.stats.rv_continuous.pdf.html) and [logpdf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.logpdf.html) functions for a number of distributions, so in practice you could use those functions instead of implementing them yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
