{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate that the p-value is uniformly distributed under $H_0$\n",
    "\n",
    "In this demonstration, we are testing whether the sampled values come from the standard normal distribution (spoiler: they do!). That is:\n",
    "\n",
    "$$H_0: \\text{the sampled values come from } N(\\mu=0,\\sigma=1)$$\n",
    "$$H_A: \\text{the sampled values do not come from } N(\\mu=0,\\sigma=1)$$\n",
    "##### Sample from the standard normal distribution \n",
    "We will start by sampling 1000 times from the standard normal distribution ($\\mu=0, \\sigma=1$) using `scipy.stats.norm.rvs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 samples from standard normal distribution\n",
    "sample = stats.norm.rvs(loc = 0, scale = 1, size = 1000)\n",
    "plt.hist(sample)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute p-values\n",
    "For each of the sampled values, we will compute the p-value, or the probability of observing a value _at least_ as extreme as the observed value under $H_0$. To do this, we can compute the cumulative density function (CDF) for the standard normal distribution at each sampled value. The CDF, evaluated at some value $x$, gives the probability that the specified distribution will take on a value less than or equal to $x$. This is the equivalent of a one-sided statistical test for the lower-tail.\n",
    "\n",
    "Note: Evaluating the survival function, or $1-CDF_{\\mu,\\sigma}(x)$, because this would be the equivalent of a one-sided statistical test for the upper tail.\n",
    "\n",
    "In this tutorial, will use the [scipy implementations](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.stats.norm.html) of the CDF and the survival function - or $\\left(1-CDF \\right)$ - for the normal distribution: `scipy.stats.norm.cdf` and `scipy.stats.norm.sf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute p-values\n",
    "pvals = stats.norm.cdf(sample, loc = 0, scale = 1)\n",
    "plt.hist(pvals)\n",
    "plt.xlabel('p')\n",
    "plt.title('p-value distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QQ-plot of p-values\n",
    "\n",
    "Here, we will show you how to plot your own QQ-plot, and visualize what a QQ-plot should look like when the observed values follow the theoretical distribution.\n",
    "\n",
    "If you recall, the QQ-plot is a plot of one set of quantiles against another. For our purposes, we will plot the quantiles of the theoretical distribution on the x-axis and the quantiles of the observed distribution on the y-axis.\n",
    "\n",
    "What's our theoretical distribution? Under $H_0$, the p-value is **uniformly distributed**. Therefore, for $N$ observed values, the quantiles of the theoretical distribution can be computed by:\n",
    "\n",
    "$$\\frac{i}{N} \\forall i\\in[0,...,N]$$\n",
    "\n",
    "To obtain the quantiles of our observed distribution, we will simply sort the values, as the p-values are essentially percentiles already!\n",
    "\n",
    "But wait - we have to make one more adjustment! Because people are usually interested in small p-values, it's more useful to plot the p-values on a log-scale. Standard practice is to use $-\\log_{10}p$. Thus, we are actually plotting the sorted values of $-\\log_{10}p$ on the y-axis, and the sorted values of $-\\log_{10}(\\text{expected})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pvals = sorted(-np.log10(pvals))\n",
    "\n",
    "plt.scatter(x = sorted(-np.log10(np.arange(1, pvals.shape[0] + 1)/pvals.shape[0])), y = sorted_pvals)\n",
    "plt.plot(sorted(-np.log10(np.arange(1, pvals.shape[0] + 1)/pvals.shape[0])), \n",
    "         sorted(-np.log10(np.arange(1, pvals.shape[0] + 1)/pvals.shape[0])), 'r-')\n",
    "plt.title(r'QQ-plot of p-values under $H_0$')\n",
    "plt.xlabel(r'$-\\log_{10}p_{expected}$')\n",
    "plt.ylabel(r'$-\\log_{10}p_{observed}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the p-value distribution look like when $H_0$ is rejected?\n",
    "\n",
    "Let's sample from a normal distribution with $\\mu=1$ this time, but test whether the mean of the sampled values is greater than that of the standard normal distribution. This is a one-sided upper tail statistical test, and we'll set a significance cutoff of $\\alpha=0.05$.\n",
    "\n",
    "$$H_0: \\mu_{sample}= 0$$\n",
    "$$H_A: \\mu_{sample}>0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 samples \n",
    "sample2 = stats.norm.rvs(loc = 1, scale = 1, size = 1000)\n",
    "plt.hist(sample2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute p-values\n",
    "\n",
    "This time, we have to use the survival function, because we are testing for how likely we are to observe a value _greater_ than the sampled value under the standard normal distribution - that is, an upper tail test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals2 = stats.norm.sf(sample2, loc = 0, scale = 1)\n",
    "plt.hist(pvals2)\n",
    "plt.xlabel('p')\n",
    "plt.title('p-value distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QQ-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pvals2 = sorted(-np.log10(pvals2))\n",
    "\n",
    "plt.scatter(x = sorted(-np.log10(np.arange(1,pvals.shape[0]+1)/pvals.shape[0])), y = sorted_pvals2)\n",
    "\n",
    "plt.plot(sorted(-np.log10(np.arange(1,pvals2.shape[0]+1)/pvals2.shape[0])), \n",
    "         sorted(-np.log10(np.arange(1,pvals2.shape[0]+1)/pvals2.shape[0])), 'r-')\n",
    "\n",
    "plt.title('QQ-plot of p-values')\n",
    "plt.xlabel(r'$-\\log_{10}p_{expected}$')\n",
    "plt.ylabel(r'$-\\log_{10}p_{observed}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Testing Correction\n",
    "\n",
    "Let's perform 1000 independent samples t-tests to test for differences in the means of two sampled groups, A and B, each of size $n=10$. \n",
    "\n",
    "Among these 1000 tests, let's predetermine that 100 of them will yield **true positives** - that is, the means of samples A and B come from two different normal distributions. For the rest of the tests, we will sample from the same normal distributions - these are **true negatives**.\n",
    "\n",
    "##### Let's start by drawing the samples:\n",
    "- for the **true positive** tests, we will sample 10 values from the standard normal distribution for group A, and 10 values from $N(\\mu=1,\\sigma=1)$ for group B\n",
    "- for the **true negative** tests, we will sample 10 values from the standard normal distribution for group A and for group B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntests = 1000\n",
    "TPtests = 100\n",
    "TNtests = Ntests - TPtests\n",
    "sampleSize = 10\n",
    "\n",
    "# True positive cases\n",
    "tp_samplesA = stats.norm.rvs(size = (TPtests, sampleSize))\n",
    "tp_samplesB = stats.norm.rvs(loc = 1, scale = 0, size = (TPtests, sampleSize))\n",
    "\n",
    "# True negative cases\n",
    "tn_samplesA = stats.norm.rvs(size = (TNtests, sampleSize))\n",
    "tn_samplesB = stats.norm.rvs(size = (TNtests, sampleSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independent samples t-test\n",
    "We will perform a two-sided, independent samples t-test to determine whether the two samples being compared have the same means:\n",
    "\n",
    "$$H_0: \\mu_A = \\mu_B$$\n",
    "$$H_A: \\mu_A \\neq \\mu_B$$\n",
    "\n",
    "This test assumes that the populations have identical variances (which is why we kept $\\sigma=1$).\n",
    "\n",
    "We will use the scipy implementation for the t-test: [`scipy.stats.ttest_ind`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the samples for groups A and B into the same arrays\n",
    "all_samplesA = np.concatenate((tp_samplesA, tn_samplesA), axis = 0)\n",
    "all_samplesB = np.concatenate((tp_samplesB, tn_samplesB), axis = 0)\n",
    "\n",
    "# perform the t-test (row-wise) \n",
    "res = stats.ttest_ind(all_samplesA, all_samplesB, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the test statistic, $t$, and corresponding $p$ for each test performed. Let's take a look at the p-value distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res.pvalue)\n",
    "plt.xlabel('p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a significance cutoff of $\\alpha = 0.05$, how many significant p-values do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSignificant = sum(res.pvalue < 0.05)\n",
    "print('%d significant results' % nSignificant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are true positives? How many are false positives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of TPs\n",
    "tp = sum(res.pvalue[:TPtests] < 0.05)\n",
    "fp = nSignificant - tp\n",
    "print('%d TPs, %d FPs' % (tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize that in the p-value distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res.pvalue, bins = np.arange(0,1.05,0.05))\n",
    "plt.hist(res.pvalue[TPtests:], bins = np.arange(0,1.05,0.05))\n",
    "plt.legend([r'$H_A$',r'$H_0$'])\n",
    "plt.xlabel('p')\n",
    "plt.axvline(x = 0.05, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the QQ-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = sorted(-np.log10(np.arange(1,Ntests+1)/Ntests)), \n",
    "            y = sorted(-np.log10(res.pvalue)))\n",
    "\n",
    "plt.plot(sorted(-np.log10(np.arange(1,Ntests+1)/Ntests)), \n",
    "         sorted(-np.log10(np.arange(1,Ntests+1)/Ntests)), 'r-')\n",
    "\n",
    "plt.title('QQ-plot of p-values')\n",
    "plt.xlabel(r'$-\\log_{10}p_{expected}$')\n",
    "plt.ylabel(r'$-\\log_{10}p_{observed}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the number of FPs consistent with the false positive rate?\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp/(fp + sum(res.pvalue[TPtests:] >= 0.05))\n",
    "print('FPR = %.3f' % fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonferroni correction\n",
    "This is a more stringent correction that adjusts the significance cutoff $\\alpha$ to account for the number of tests.\n",
    "$$\\alpha_{\\text{Bonferroni}} = \\frac{\\alpha}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaBonf = 0.05/Ntests\n",
    "bonfSignificant = sum(res.pvalue < alphaBonf)\n",
    "print('%d significant results after Bonferroni correction' % bonfSignificant)\n",
    "\n",
    "bonfTP = sum(res.pvalue[:TPtests] < alphaBonf)\n",
    "print('%d TPs after Bonferroni correction' % bonfTP)\n",
    "\n",
    "bonfFP = sum(res.pvalue[TPtests:] < alphaBonf)\n",
    "bonfFPR = bonfFP/(bonfFP + sum(res.pvalue[TPtests:] >= alphaBonf))\n",
    "print('%d FPs after Bonferroni correction, FPR = %.3f' % (bonfFP, bonfFPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the number of true positives change after Bonferroni correction? How about the number of false positives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
